## 作业二：用梯度下降求极小值

“以$f(x)=cos(x)+sin(2x)$函数为例，探究梯度下降算法中**学习率**对算法性能的影响。要求写出要给写出计算方法、相应python代码、计算结果及完成本次探究的心得”

使用python，利用梯度下降方法对函数$f(x)=cos(x)+sin(2x)求最小值，并绘制出梯度下降的过程图像，给出原理和具体代码

### 资料收集

#### [资料一](https://blog.csdn.net/qq_41800366/article/details/86583789?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167887802016800211561504%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=167887802016800211561504&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-86583789-null-null.142^v73^control,201^v4^add_ask,239^v2^insert_chatgpt&utm_term=%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D&spm=1018.2226.3001.4187)

梯度下降的基本过程就和下山的场景很类似。

首先，我们有一个可微分的函数。这个函数就代表着一座山。我们的目标就是找到这个函数的最小值，也就是山底。根据之前的场景假设，最快的下山的方式就是找到当前位置最陡峭的方向，然后沿着此方向向下走，对应到函数中，就是找到给定点的梯度 ，然后朝着梯度相反的方向，就能让函数值下降的最快！因为**梯度的方向就是函数之变化最快的方向** 


**梯度**是微积分中一个很重要的概念，之前提到过梯度的意义

- 在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率
- **在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向**

公式

$Θ^1=Θ^0+α▽J(Θ)→evaluatedatΘ^0$

此公式的意义是：J是关于Θ的一个函数，我们当前所处的位置为Θ0点，要从这个点走到J的最小值点，也就是山底。首先我们先确定前进的方向，也就是梯度的反向，然后走一段距离的步长，也就是**α**，走完这个段步长，就到达了$Θ^1$这个点！

![20190121203434245](%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.assets/20190121203434245.png)



α

α在梯度下降算法中被称作为学习率或者步长，意味着我们可以通过α来控制每一步走的距离，以保证不要走太快，错过了最低点。同时也要保证不要走的太慢。所以α的选择在梯度下降法中往往是很重要的！α不能太大也不能太小，太小的话，可能导致迟迟走不到最低点，太大的话，会导致错过最低点！
————————————————
梯度要乘以一个负号

梯度前加一个负号，就意味着朝着梯度相反的方向前进！我们在前文提到，梯度的方向实际就是函数在此点上升最快的方向！而我们需要朝着下降最快的方向走，自然就是负的梯度的方向，所以此处需要加上负号；**那么如果时上坡，也就是梯度上升算法，当然就不需要添加负号了。**

#### py绘图实现方法

[知乎](https://zhuanlan.zhihu.com/p/109245779)

#### csdn-chat

```python
import numpy as np
import matplotlib.pyplot as plt

# 定义目标函数
def f(x):
    return x**2 + 2*x + 1

# 定义目标函数的导数
def df(x):
    return 2*x + 2

# 定义梯度下降函数
def gradient_descent(x, lr, epochs):
    losses = []
    for i in range(epochs):
        # 计算梯度
        grad = df(x)
        # 更新参数
        x = x - lr * grad
        # 计算损失
        loss = f(x)
        losses.append(loss)
    return x, losses

# 初始化参数
x = -5
lr = 0.1
epochs = 50

# 进行梯度下降
x_min, losses = gradient_descent(x, lr, epochs)

# 画出损失函数曲线
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Gradient Descent')
plt.show()
```

```python
import numpy as np
import matplotlib.pyplot as plt

def f(x):
    return np.cos(x) * np.sin(2 * x)

def df(x):
    return -np.sin(x) * np.sin(2 * x) + 2 * np.cos(x) * np.cos(2 * x)

def gradient_descent(x, learning_rate, num_iterations):
    history = []
    for i in range(num_iterations):
        grad = df(x)
        x = x - learning_rate * grad
        history.append(f(x))
    return x, history

x0 = 0.5
learning_rate = 0.01
num_iterations = 100

x, history = gradient_descent(x0, learning_rate, num_iterations)

print("最小值为：", f(x))

plt.plot(history)
plt.xlabel("迭代次数")
plt.ylabel("函数值")
plt.show()
```





对于函数$f(x)=cos(x) sin(2x)$，我们可以使用梯度下降法来求解其最小值。梯度下降法是一种常用的优化算法，其基本思想是通过不断迭代来逐步优化目标函数，使其达到最小值。

具体来说，我们可以先求出$f(x)$的导数，即$f'(x)=-sin(x)sin(2x)+2cos(x)cos(2x)$。然后，我们可以随机初始化一个$x$的值，然后不断迭代，每次更新$x$的值，直到达到最小值。

```python
import numpy as np
import matplotlib.pyplot as plt 
# 定义目标函数
def f(x):
    return np.cos(x) * np.sin(2*x)
# 定义目标函数的导数
def df(x):
    return -np.sin(x)*np.sin(2*x) + 2*np.cos(x)*np.cos(2*x)
# 定义梯度下降函数
def gradient_descent(x, lr, epochs):
    x_history = [x] # 记录每次迭代的x值
    for i in range(epochs):
        x = x - lr * df(x) # 更新x的值
        x_history.append(x)
    return x_history
# 初始化x的值
x_init = 0.5

# 设置学习率和迭代次数
lr = 0.1
epochs = 50
# 进行梯度下降
x_history = gradient_descent(x_init, lr, epochs)

# 绘制函数图像和梯度下降过程图像
x = np.linspace(-2*np.pi, 2*np.pi, 1000)
y = f(x)
plt.plot(x, y)
plt.plot(x_history, f(np.array(x_history)), 'ro')
plt.show()

```



```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

def f(x):
    return np.cos(x) * np.sin(2*x)

def grad_f(x):
    return -np.sin(x)*np.sin(2*x) + 2*np.cos(x)*np.cos(2*x)

def gradient_descent(x0, eta, n_iterations):
    x = x0
    x_history = [x]
    for i in range(n_iterations):
        x = x - eta * grad_f(x)
        x_history.append(x)
    return x_history

x0 = 0
eta = 0.1
n_iterations = 50
x_history = gradient_descent(x0, eta, n_iterations)

fig, ax = plt.subplots()
x_vals = np.linspace(-5, 5, 100)
y_vals = f(x_vals)
ax.plot(x_vals, y_vals)

line, = ax.plot([], [], 'ro')

def init():
    line.set_data([], [])
    return line,

def update(frame):
    x = x_history[frame]
    y = f(x)
    line.set_data(x, y)
    return line,

ani = FuncAnimation(fig, update, frames=len(x_history), init_func=init, blit=True)
plt.show()
```

在这段代码中，我们首先定义了函数$f(x)$和它的导数$grad_f(x)$。然后，我们实现了梯度下降算法，其中$x0$是初始值，$eta$是学习率，$n_iterations$是迭代次数。最后，我们使用Matplotlib的动画功能来绘制梯度下降的过程。



3.

梯度下降法是一种常用的优化算法，用于求解函数的最小值。其基本思想是通过不断迭代，沿着函数的负梯度方向逐步逼近最小值点。在每一次迭代中，我们需要计算函数的梯度，并根据梯度的方向和大小来更新参数。具体来说，对于函数$f(x)$，我们可以通过以下公式来更新参数$x$：

$x_{t+1} = x_t - \alpha \nabla f(x_t)$

其中，$x_t$表示第$t$次迭代时的参数值，$\alpha$表示学习率，$\nabla f(x_t)$表示函数$f(x_t)$在$x_t$处的梯度。

代码：

首先，我们需要导入必要的库：

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
```

接下来，定义函数$f(x)$：

```python
def f(x):
    return np.cos(x) * np.sin(2 * x)
```

然后，定义梯度下降算法：

```python
def gradient_descent(x, lr, epochs):
    # 初始化参数
    x_history = [x]
    y_history = [f(x)]
    # 迭代更新参数
    for i in range(epochs):
        # 计算梯度
        grad = (f(x + 0.0001) - f(x)) / 0.0001
        # 更新参数
        x = x - lr * grad
        # 记录参数和函数值
        x_history.append(x)
        y_history.append(f(x))
    return x_history, y_history
```

最后，我们可以使用Matplotlib的动画功能来绘制梯度下降的过程：

```python
# 初始化参数
x_init = 2
lr = 0.1
epochs = 50

# 运行梯度下降算法
x_history, y_history = gradient_descent(x_init, lr, epochs)

# 绘制动画
fig, ax = plt.subplots()
ax.set_xlim(-5, 5)
ax.set_ylim(-2, 2)
line, = ax.plot([], [], lw=2)

def update(frame):
    x = x_history[frame]
    y = y_history[frame]
    line.set_data([x], [y])
    return line,

ani = FuncAnimation(fig, update, frames=len(x_history), blit=True)
plt.show()
```

注释：

1. `import numpy as np`：导入NumPy库，用于数值计算。
2. `import matplotlib.pyplot as plt`：导入Matplotlib库，用于绘图。
3. `from matplotlib.animation import FuncAnimation`：导入Matplotlib的动画功能。
4. `def f(x):`：定义函数$f(x)$。
5. `def gradient_descent(x, lr, epochs):`：定义梯度下降算法。
6. `x_history = [x]`：初始化参数$x$的历史记录。
7. `y_history = [f(x)]`：初始化函数值$f(x)$的历史记录。
8. `for i in range(epochs):`：迭代更新参数。
9. `grad = (f(x + 0.0001) - f(x)) / 0.0001`：计算梯度。
10. `x = x - lr * grad`：更新参数。
11. `x_history.append(x)`：记录参数$x$的历史记录。
12. `y_history.append(f(x))`：记录函数值$f(x)$的历史记录。
13. `fig, ax = plt.subplots()`：创建图形窗口和坐标系。
14. `ax.set_xlim(-5, 5)`：设置$x$轴的范围。
15. `ax.set_ylim(-2, 2)`：设置$y$轴的范围。
16. `line, = ax.plot([], [], lw=2)`：创建空的曲线对象。
17. `def update(frame):`：定义动画更新函数。
18. `x = x_history[frame]`：获取当前参数$x$的值。
19. `y = y_history[frame]`：获取当前函数值$f(x)$的值。
20. `line.set_data([x], [y])`：更新曲线对象的数据。
21. `return line,`：返回曲线对象。
22. `ani = FuncAnimation(fig, update, frames=len(x_history), blit=True)`：创建动画对象。
23. `plt.show()`：显示动画。

-----



代码中首先定义了目标函数 $f(x)$ 和其导函数 $df(x)$，然后实现了梯度下降算法 gradient_descent，该函数接受起始点 $x_0$，学习率 learning_rate，和最大迭代次数 max_iter 作为参数，返回梯度下降过程中每一步的 $x$ 和 $f(x)$ 值的列表。最后，代码将函数图像和梯度下降过程图像绘制出来，并用 animation 实现动态更新梯度下降过程图像的效果。

需要注意的是，梯度下降算法可能会陷入局部最小值，因此需要多次运行算法，尝试不同的起始点 $x_0$ 和学习率 learning_rate，以找到全局最小值。

chat

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation


# 定义目标函数
def f(x):
    return np.cos(x) + np.sin(2 * x)


# 定义目标函数的导函数
def df(x):
    return -np.sin(x) + 2 * np.cos(2 * x)


# 定义梯度下降算法
def gradient_descent(x0, learning_rate, max_iter):
    # 初始化
    x = x0
    x_history = [x]
    y_history = [f(x)]
    # 迭代
    for i in range(max_iter):
        x = x - learning_rate * df(x)
        x_history.append(x)
        y_history.append(f(x))
        # 打印极小值
        print("第", i + 1, "次的极小值为：", y_history[-1])
    return x_history, y_history


# 执行梯度下降算法
x0 = -5
learning_rate = 0.1
# learning_rate = np.linspace(0.01, 0.1, 10);
max_iter = 20

x_history, y_history = gradient_descent(x0, learning_rate, max_iter)

# 绘制函数图像

x_plot = np.linspace(-2 * np.pi, 2 * np.pi, 1000)
y_plot = f(x_plot)
plt.plot(x_plot, y_plot, label='function')

# 绘制梯度下降过程图像
line, = plt.plot([], [], 'ro', label='gradient descent')


# 更新图像
def update(i):
    line.set_data(x_history[:i + 1], y_history[:i + 1])
    return line,


# 创建动画
anim = FuncAnimation(plt.gcf(), update, frames=len(x_history), interval=100, blit=True)

# 显示图像
plt.legend()
plt.show()

```



`x_grad = np.linspace(1, len(grad), len(grad))`

[csdn-np.linspace](https://blog.csdn.net/neweastsun/article/details/99676029?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167929950816800180670048%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=167929950816800180670048&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-99676029-null-null.142^v74^control,201^v4^add_ask,239^v2^insert_chatgpt&utm_term=%20np.linspace&spm=1018.2226.3001.4187)

[【学习率】梯度下降学习率的设定策略](https://blog.csdn.net/lj2048/article/details/107066281?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167936643916800215073648%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=167936643916800215073648&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-4-107066281-null-null.142^v74^control,201^v4^add_ask,239^v2^insert_chatgpt&utm_term=%E5%AD%A6%E4%B9%A0%E7%8E%87%E5%AF%B9%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E5%BD%B1%E5%93%8D&spm=1018.2226.3001.4187)

