提示：数据可以在网上下载复旦新闻语料库或自行爬取，使用nlp以及jieba分词



# [朴素贝叶斯算法原理+Python实现](https://blog.csdn.net/zcz0101/article/details/109577494)



[语料库](https://zhuanlan.zhihu.com/p/401920621)

[FD语料库](https://languageresources.github.io/2018/05/13/%E5%8D%A2%E6%A2%A6%E4%BE%9D_%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E8%AF%AD%E6%96%99%E5%BA%93%EF%BC%88%E5%A4%8D%E6%97%A6%EF%BC%89%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E9%9B%86/)



`pip install -i https://pypi.tuna.tsinghua.edu.cn/simple jieba`



```python
`pip install -i https://pypi.tuna.tsinghua.edu.cn/simple jieba`
```



### 预处理



```cpp
import jieba
import codecs
import re
from collections import Counter

# 读取语料库，按行读取
def read_corpus(path):
    with codecs.open(path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    return [line.strip() for line in lines]

# 去除停用词
def stop_words(path):
    with codecs.open(path, 'r', encoding='utf-8') as f:
        return set([line.strip() for line in f])

# 分词，去停用词，统计词频，返回字典
def build_dict(corpus_path, stopwords_path):
    # 读取语料库和停用词
    corpus = read_corpus(corpus_path)
    stopwords = stop_words(stopwords_path)

    # 对每个文本进行分词，去停用词，统计词频
    word_freq = Counter()
    for sentence in corpus:
        words = jieba.cut(sentence)
        for word in words:
            if word not in stopwords:
                word_freq[word] += 1

    # 将词频排序，取前5000个高频词作为词典
    word_freq = word_freq.most_common(5000)

    # 构建词典，编号从1开始
    word_dict = {w[0]: index + 1 for (index, w) in enumerate(word_freq)}

    return word_dict

# 将字典写入文件
def write_dict(word_dict, path):
    with codecs.open(path, 'w', encoding='utf-8') as f:
        for w in word_dict:
            f.write(w + '\t' + str(word_dict[w]) + '\n')

# 测试代码
corpus_path = 'corpus.txt'
stopwords_path = 'stopwords.txt'
word_dict = build_dict(corpus_path, stopwords_path)
write_dict(word_dict, 'dict.txt')

```

解释：

- `read_corpus(path)`：读取语料库文件，按行读取，返回一个字符串列表。
- `stop_words(path)`：读取停用词文件，返回一个停用词集合。
- `build_dict(corpus_path, stopwords_path)`：读取语料库文件和停用词文件，对每个文本进行分词，去停用词，统计词频，取前5000个高频词作为词典，返回一个字典，将词汇与序号相对应。
- `write_dict(word_dict, path)`：将字典写入文件，每一行表示一个词汇和它对应的序号，以`\t`分隔。

在这个例子中，我们将分词得到的字典写入了一个文件`dict.txt`中，可以在后续的代码中使用它来将文本数据转换成向量。



运行结果：

```py
Building prefix dict from the default dictionary ...
Dumping model to file cache E:\tmpdir\Temp\jieba.cache
Loading model cost 0.589 seconds.
Prefix dict has been built successfully.

```



解释：

>这段输出是jieba分词库在初始化时的一些信息，包括构建词典前缀和加载模型等过程。
>
>具体来说，它包含以下步骤：
>
>1. "Building prefix dict from the default dictionary ..."：表示正在从默认的词典构建前缀字典。
>
>2. "Dumping model to file cache E:\tmpdir\Temp\jieba.cache"：表示将模型数据转储到文件缓存中，这是为了加快下次加载模型的速度。文件路径可能会因实际情况而异。
>
>3. "Loading model cost 0.589 seconds."：表示加载模型所花费的时间，这里是0.589秒。加载模型是为了使用预训练的模型数据进行分词操作。
>
>4. "Prefix dict has been built successfully."：表示前缀字典构建成功，jieba分词库已准备就绪，可以开始进行分词操作了。
>
>这些输出信息表明jieba分词库已成功初始化，并且可以使用它来进行文本分词操作了。
>
>



接下来，我们可以使用上一步得到的字典来将文本数据转换成向量。这里我们使用one-hot编码的方式，将每个文本表示为一个高维稀疏向量，其中每个维度对应一个词汇，如果该词汇在文本中出现，则对应维度的值为1，否则为0。

具体的代码如下：

```python
import codecs
import numpy as np

# 读取字典文件，返回字典
def read_dict(path):
    with codecs.open(path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    return {line.strip().split('\t')[0]: int(line.strip().split('\t')[1]) for line in lines}

# 将文本转换成向量，使用one-hot编码
def text2vec(text, word_dict):
    vec = np.zeros(len(word_dict))
    words = jieba.cut(text)
    for word in words:
        if word in word_dict:
            vec[word_dict[word] - 1] = 1
    return vec

# 将文本数据转换成向量
def data2vec(data_path, word_dict):
    with codecs.open(data_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    vecs = []
    for line in lines:
        vec = text2vec(line.strip(), word_dict)
        vecs.append(vec)
    return np.array(vecs)

# 测试代码
word_dict = read_dict('dict.txt')
vecs = data2vec('corpus.txt', word_dict)
print(vecs.shape)

```

解释：

- `read_dict(path)`：读取字典文件，返回一个字典，将词汇与序号相对应。
- `text2vec(text, word_dict)`：将一个文本转换成向量，使用one-hot编码，返回一个numpy数组。
- `data2vec(data_path, word_dict)`：读取文本数据文件，将每个文本转换成向量，返回一个numpy数组。
- `vecs.shape`：打印向量矩阵的形状。

在这个例子中，我们使用了`numpy`库来处理向量，将每个文本转换成一个向量。最终得到的是一个numpy数组，每行对应一个文本，每列对应一个词汇。



现在，我们已经得到了预处理后的文本数据的向量表示。接下来，我们可以使用朴素贝叶斯分类算法对文本进行分类。

在这里，我们使用`scikit-learn`库中的`MultinomialNB`类来实现朴素贝叶斯分类器。具体的代码如下：



----



其中，我们定义了四个函数：

- `get_stop_words(stop_word_file)`：加载停用词。
- `read_dict(dict_file)`：读取词典。
- `text2vec(text, word_dict)`：将一条文本转换为向量表示。
- `data2vec(data_file, word_dict)`：将数据集转换为向量集。

在`main`函数中，我们首先加载停用词和词典，然后将训练集和测试集转换为向量集。接着，我们创建一个`MultinomialNB`对象作为朴素贝叶斯分类器，并使用训练集训练分类器。最后，在测试集上评估分类器的性能，并输出准确率。

需要注意的是，在实际应用中，我们需要使用大规模的数据集来训练分类器，并在测试集上进行交叉验证，以评估分类器的性能。此外，我们还可以使用其他的分类算法，比如支持向量机（SVM）、随机森林等。

---



在上面的代码中，我们使用了朴素贝叶斯分类算法来对文本进行分类。为了避免出现概率为0的情况，我们在计算每个词的条件概率时，使用了拉普拉斯平滑。具体来说，我们将每个词出现的次数加上一个平滑因子1，再除以该词在整个文档集中出现的总次数加上文档集的词汇量，得到该词在文档集中出现的条件概率。

下面是实现拉普拉斯平滑的代码

```cpp
import jieba
import numpy as np
from sklearn.naive_bayes import MultinomialNB

# 加载停用词
def get_stop_words(stop_word_file):
    with open(stop_word_file, 'r', encoding='utf-8') as f:
        stop_words = [line.strip() for line in f]
    return stop_words

# 读取词典
def read_dict(dict_file):
    with open(dict_file, 'r', encoding='utf-8') as f:
        word_dict = [line.strip() for line in f]
    return word_dict

# 将文本转换为向量
def text2vec(text, word_dict):
    # 分词、去停用词
    words = [word for word in jieba.cut(text) if word.strip() and word not in stop_words]
    # 构建向量
    vec = np.zeros(len(word_dict), dtype=int)
    for word in words:
        if word in word_dict:
            vec[word_dict.index(word)] += 1
    return vec

# 将数据集转换为向量集
def data2vec(data_file, word_dict):
    with open(data_file, 'r', encoding='utf-8') as f:
        texts = [line.strip() for line in f]
    vecs = [text2vec(text, word_dict) for text in texts]
    return vecs

# 加载数据集并进行预处理
stop_words = get_stop_words('stop_words.txt')
word_dict = read_dict('dict.txt')
train_vecs = data2vec('train.txt', word_dict)
train_labels = np.loadtxt('train_labels.txt', dtype=int)
test_vecs = data2vec('test.txt', word_dict)
test_labels = np.loadtxt('test_labels.txt', dtype=int)

# 训练朴素贝叶斯分类器
clf = MultinomialNB(alpha=1.0, fit_prior=True)
clf.fit(train_vecs, train_labels)

# 在测试集上评估分类器的性能
acc = clf.score(test_vecs, test_labels)
print('Accuracy:', acc)

```





```python
import numpy as np

class NaiveBayes:
    def fit(self, X, y):
        self.num_examples, self.num_features = X.shape
        self.classes = np.unique(y)
        self.num_classes = len(self.classes)
        self.mean = np.zeros((self.num_classes, self.num_features), dtype=np.float64)
        self.var = np.zeros((self.num_classes, self.num_features), dtype=np.float64)
        self.priors =  np.zeros(self.num_classes, dtype=np.float64)

        for c in self.classes:
            X_c = X[c==y]
            self.mean[c, :] = X_c.mean(axis=0)
            self.var[c, :] = X_c.var(axis=0)
            self.priors[c] = X_c.shape[0] / float(self.num_examples)

    def predict(self, X):
        y_pred = [self._predict(x) for x in X]
        return y_pred

    def _predict(self, x):
        posteriors = []

        for idx, c in enumerate(self.classes):
            prior = np.log(self.priors[idx])
            class_conditional = np.sum(np.log(self._pdf(idx, x)))
            posterior = prior + class_conditional
            posteriors.append(posterior)

        return self.classes[np.argmax(posteriors)]

    def _pdf(self, class_idx, x):
        mean = self.mean[class_idx]
        var = self.var[class_idx]
        numerator = np.exp(-(x-mean)**2 / (2 * var))
        denominator = np.sqrt(2 * np.pi * var)
        return numerator / denominator
```

























- 请用给出朴素贝叶斯分类算法的python代码实现，并给出每一步骤的解释

  点击复制后，将打开C知道体验页

- <img src="https://csdnimg.cn/release/searchv2-fe/img/icon-menu-chat.e7c66de4.png" alt="img" style="zoom:25%;" />

  首先，我们需要导入一些需要用到的库：

  ```python
  import numpy as np
  from collections import defaultdict
  ```

  接下来，我们定义一个朴素贝叶斯分类器的类：

  ```python
  class NaiveBayesClassifier:
      def __init__(self):
          self.classes = None
          self.prior = None
          self.cond_prob = defaultdict(lambda: defaultdict(int))
      
      def fit(self, X, y):
          # 计算类别数量
          self.classes = np.unique(y)
          
          # 计算每个类别的先验概率
          self.prior = np.array([(y == c).sum() / float(len(y)) for c in self.classes])
          
          # 计算每个特征的条件概率
          for i, x in enumerate(X):
              c = y[i]
              for j, f in enumerate(x):
                  self.cond_prob[c][j, f] += 1
          
          for c in self.classes:
              for j in range(X.shape[1]):
                  total = float(sum(self.cond_prob[c][j].values()))
                  for f in self.cond_prob[c][j]:
                      self.cond_prob[c][j, f] /= total
      
      def predict(self, X):
          # 计算每个样本的后验概率
          posteriors = []
          for x in X:
              posterior = self.prior.copy()
              for j, f in enumerate(x):
                  for c in self.classes:
                      posterior[c] *= self.cond_prob[c][j, f]
              posteriors.append(posterior)
          
          # 返回后验概率最大的类别
          return np.array([self.classes[np.argmax(p)] for p in posteriors])
  ```

  现在，我们来一步步解释每一步：

  1. `__init__` 方法初始化朴素贝叶斯分类器，包括类别和先验概率。
  2. `fit` 方法拟合数据，计算每个类别的先验概率和每个特征的条件概率。
  3. `predict` 方法使用拟合好的模型对新数据进行分类，计算每个样本的后验概率并返回后验概率最大的类别。

  至此，这就是一个朴素贝叶斯分类算法的 Python 实现，它可以用于对文本、图像等数据进行分类。





#### 5.10



pre

```python
import os
import jieba
import codecs
import chardet
from collections import Counter

# 读取语料库，按行读取
def read_corpus(path, encoding='utf-8'):
    with open(path, 'rb') as f:
        raw_data = f.read()
        detected_encoding = chardet.detect(raw_data)['encoding']
        encoding = detected_encoding if detected_encoding is not None else encoding
    with codecs.open(path, 'r', encoding=encoding, errors='ignore') as f:
        lines = f.readlines()
    return [line.strip() for line in lines]

# 去除停用词
def stop_words(path, encoding='utf-8'):
    with codecs.open(path, 'r', encoding=encoding) as f:
        return set([line.strip() for line in f])

# 分词，去停用词，统计词频，返回字典
def build_dict(corpus_folder, stopwords_path):
    # 遍历语料库文件夹中的所有文件
    corpus = []
    for root, dirs, files in os.walk(corpus_folder):
        for file_name in files:
            file_path = os.path.join(root, file_name)
            if os.path.isfile(file_path):
                corpus.extend(read_corpus(file_path))

    stopwords = stop_words(stopwords_path)

    # 对每个文本进行分词，去停用词，统计词频
    word_freq = Counter()
    for sentence in corpus:
        words = jieba.cut(sentence)
        for word in words:
            if word not in stopwords:
                word_freq[word] += 1

    # 将词频排序，取前5000个高频词作为词典
    word_freq = word_freq.most_common(5000)

    # 构建词典，编号从1开始
    word_dict = {w[0]: index + 1 for (index, w) in enumerate(word_freq)}

    return word_dict

# 将字典写入文件
def write_dict(word_dict, path, encoding='utf-8'):
    with codecs.open(path, 'w', encoding=encoding) as f:
        for w in word_dict:
            f.write(w + '\t' + str(word_dict[w]) + '\n')

# 测试代码
corpus_folder = 'D:\\data\\C3-Art'
stopwords_path = 'stopwords.txt'
word_dict = build_dict(corpus_folder, stopwords_path)
write_dict(word_dict, 'dict.txt')

```



```python
import jieba
import numpy as np
from sklearn.naive_bayes import MultinomialNB

# 加载停用词
def get_stop_words(stop_word_file):
    with open(stop_word_file, 'r', encoding='gbk') as f:
        stop_words = [line.strip() for line in f]
    return stop_words

# 读取词典
def read_dict(dict_file):
    with open(dict_file, 'r', encoding='gbk') as f:
        word_dict = [line.strip() for line in f]
    return word_dict

# 将文本转换为向量
def text2vec(text, word_dict):
    # 分词、去停用词
    words = [word for word in jieba.cut(text) if word.strip() and word not in stop_words]
    # 构建向量
    vec = np.zeros(len(word_dict), dtype=int)
    for word in words:
        if word in word_dict:
            vec[word_dict.index(word)] += 1
    return vec

# 将数据集转换为向量集
def data2vec(data_file, word_dict):
    with open(data_file, 'r', encoding='gbk') as f:
        texts = [line.strip() for line in f]
    vecs = [text2vec(text, word_dict) for text in texts]
    return vecs

# 加载数据集并进行预处理
stop_words = get_stop_words('stopwords.txt')
word_dict = read_dict('dict.txt')
train_vecs = data2vec('train.txt', word_dict)
train_labels = np.loadtxt('train_labels.txt', dtype=int)
test_vecs = data2vec('test.txt', word_dict)
test_labels = np.loadtxt('test_labels.txt', dtype=int)

# 训练朴素贝叶斯分类器
clf = MultinomialNB(alpha=1.0, fit_prior=True)
clf.fit(train_vecs, train_labels)

# 在测试集上评估分类器的性能
acc = clf.score(test_vecs, test_labels)
print('Accuracy:', acc)

```



划分测试组和训练组



```python
import os
import random
import shutil

# 定义语料库文件夹路径和训练集、测试集文件夹路径
corpus_folder = '语料库文件夹路径'
train_folder = '训练集文件夹路径'
test_folder = '测试集文件夹路径'

# 确定训练集和测试集的比例
train_ratio = 0.8  # 训练集比例，可根据需求调整

# 遍历每个领域的新闻文件夹
for domain_folder in os.listdir(corpus_folder):
    domain_path = os.path.join(corpus_folder, domain_folder)
    
    # 获取该领域新闻文件夹下的所有文件
    file_list = os.listdir(domain_path)
    
    # 打乱文件顺序
    random.shuffle(file_list)
    
    # 划分训练集和测试集
    train_size = int(train_ratio * len(file_list))
    train_files = file_list[:train_size]
    test_files = file_list[train_size:]
    
    # 将文件复制到相应的训练集和测试集文件夹中
    for file in train_files:
        src_path = os.path.join(domain_path, file)
        dst_path = os.path.join(train_folder, domain_folder, file)
        shutil.copyfile(src_path, dst_path)
    
    for file in test_files:
        src_path = os.path.join(domain_path, file)
        dst_path = os.path.join(test_folder, domain_folder, file)
        shutil.copyfile(src_path, dst_path)

```





#### proposal

```markdown
"Abstract:
In this study, we address the problem of text classification using the Naive Bayes algorithm. Text classification is a fundamental task in natural language processing and has various real-world applications. The objective of our research is to evaluate the effectiveness of the Naive Bayes algorithm in classifying text documents. We implemented the algorithm using a large dataset of news articles and compared its performance against other state-of-the-art classifiers. Our experiments show that the Naive Bayes algorithm achieves competitive accuracy and computational efficiency, making it a suitable choice for large-scale text classification tasks. We also discuss the limitations of the algorithm and suggest potential areas for future research, such as incorporating semantic features and exploring ensemble methods. Overall, this study contributes to the understanding of the Naive Bayes algorithm's application in text classification and provides insights for further improvements in this field."



[1] Rish, I. (2001). An empirical study of the Naive Bayes classifier. In IJCAI 2001 workshop on empirical methods in artificial intelligence (pp. 41-46).↳

[2] Domingos, P., & Pazzani, M. (1997). On the optimality of the simple Bayesian classifier under zero-one loss. Machine Learning, 29(2-3), 103-130.
[3] Kazmierska, J., & Malicki, J. (2008). Application of the Naive Bayesian classifier to optimize treatment decisions. Radiotherapy and Oncology, 86(2), 211-216.
[4] Wang, Q., Garrity, G. M., Tiedje, J. M., & Cole, J. R. (2007). Naive Bayesian classifier for rapid assignment of rRNA sequences into the new bacterial taxonomy. Applied and Environmental Microbiology, 73(16), 5261-5267.
[5] Sahami, M., Dumais, S., Heckerman, D., & Horvitz, E. (1998). A Bayesian approach to filtering junk e-mail. In Learning for Text Categorization: Papers from the 1998 workshop (Vol. 62, pp. 98-105).

In summary, the Naive Bayes algorithm demonstrates promising results in text classification tasks, but further research is needed to address its limitations and enhance its performance in more complex scenarios. By advancing our understanding of the algorithm's strengths and weaknesses, this study paves the way for more accurate and efficient text classification techniques.
```

ref

[The Naive Bayes algorithm](https://towardsdatascience.com/text-classification-using-naive-bayes-theory-a-working-example-2ef4b7eb7d5a)

```asm
.model small
.stack 100h

.data
AS db 31h, 36h, 33h, 32h, 38h, 37h, 35h

.code
main proc
    mov ax, @data
    mov ds, ax
    
    mov si, offset AS
    mov al, [si]
    
    mov cx, 6
    mov bx, si
    
    mov si, offset AS+1
    
    mov dl, [si]
    mov [bx], dl
    
    inc si
    inc bx
    
    loop main
    
    mov [bx], al
    
    mov ah, 4Ch
    int 21h
main endp

end main

```

