# [**决策树算法**--CART分类树算法](https://blog.csdn.net/Amy9_Miss/article/details/106043390?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_utm_term~default-4-106043390-blog-120992980.235^v35^pc_relevant_increate_t0_download_v2_base&spm=1001.2101.3001.4242.3&utm_relevant_index=7)

ID3和C4.5算法，生成的决策树是多叉树，只能处理分类不能处理回归。而CART（classification and regression tree）分类回归树算法，既可用于分类也可用于回归。 分类树的输出是样本的类别， 回归树的输出是一个实数。

CART算法步骤

1.特征选择；
2.递归建立决策树；
3.决策树剪枝；

**CART分类树算法**

ID3中使用了信息增益选择特征，增益大优先选择。C4.5中，采用信息增益率选择特征，减少因特征值多导致信息增益大的问题。CART分类树算法使用基尼系数选择特征，基尼系数代表了模型的不纯度，基尼系数越小，不纯度越低，特征越好。这和信息增益（率）相反。
基尼系数

数据集D的纯度可用基尼值来度量
$$
Gini(D)=\sum_{i=1}^{n}{p(x_i)*(1-p(x_i))}=1-\sum_{i=1}^{n}{p(x_i)^2}
$$
其中， p ( x i )是分类 x i 出现的概率，n是分类的数目。Gini(D)反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率。因此，Gini(D)越小，则数据集D的纯度越高。

对于样本D，个数为|D|，根据特征A 是否取某一可能值a，把样本D分成两部分 D1和D2 。所以CART分类树算法建立起来的是二叉树，而不是多叉树。



