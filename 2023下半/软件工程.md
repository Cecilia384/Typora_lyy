### [软件工程学习笔记](https://blog.csdn.net/qq_39183034/article/details/122793073?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169986057116800226529736%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169986057116800226529736&biz_id=0&spm=1018.2226.3001.4187)



### [软件工程知识点总结](https://blog.csdn.net/CherryChenieth/article/details/124097054?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522170048622716800182190087%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=170048622716800182190087&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-124097054-null-null.142^v96^pc_search_result_base2&utm_term=%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B&spm=1018.2226.3001.4187)！！！



### [软件工程期末复习](https://blog.csdn.net/m0_57099044/article/details/121640305)



## 试题

[期末试题1](https://www.cnblogs.com/qyf2199/p/12104922.html)

[软件工程期末考试题库（超全）](https://blog.csdn.net/weixin_46232841/article/details/111387192)

[软件工程期末试题及答案（史上最全）](https://blog.csdn.net/weixin_43474701/article/details/118760936)





### 12.13

爬虫代码

```python
import csv
import requests
from bs4 import BeautifulSoup

def get_movie_list():
    url = "https://movie.douban.com"  # 豆瓣正在热映电影页面的URL
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1;WOW64) AppleWebKit/537.36 (KHTML,like GeCKO) Chrome/45.0.2454.85 Safari/537.36 115Broswer/6.0.3',
        'Referer': 'https://movie.douban.com/',
        'Connection': 'keep-alive'
    }

    response = requests.get(url, headers=headers)  # 请求URL
    soup = BeautifulSoup(response.text, 'html.parser')  # 解析网页

    movie_list = soup.select('#screening .ui-slide-item')  # 找到正在热映电影的li
    return movie_list

def get_movie_info(movie_item, headers):
    movie_url = movie_item.select_one(".poster a")["href"]  # 获取电影主页的URL
    movie_response = requests.get(movie_url, headers=headers)  # 请求电影主页
    movie_soup = BeautifulSoup(movie_response.text, 'html.parser')  # 解析电影主页

    title = movie_soup.select_one("[property='v:itemreviewed']").get_text(strip=True)  # 电影名称
    director = movie_soup.select_one(".attrs [rel='v:directedBy']").get_text(strip=True)  # 导演

    writer_tag = movie_soup.find("span", text="编剧")
    if writer_tag:
        writer = writer_tag.find_next("span", class_="attrs").get_text(strip=True)
    else:
        writer = "无编剧信息"

    actors_list = movie_soup.select(".actor .attrs a")  # 主演
    actors = " / ".join([actor.get_text(strip=True) for actor in actors_list[:5]])

    genres = movie_soup.select("[property='v:genre']")  # 类型
    genres = " / ".join([genre.get_text(strip=True) for genre in genres])

    score = movie_soup.select_one("strong[property='v:average']").get_text(strip=True)  # 豆瓣评分
    if score == "":
        score = "暂无"

    vote_count = movie_soup.select_one("span[property='v:votes']")  # 评价人数
    vote_count = vote_count.get_text(strip=True) if vote_count else "暂无"

    return [title, director, writer, actors, genres, score, vote_count]

def write_to_csv(data):
    filename = "hot_movies.csv"
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['电影名称', '导演', '编剧', '主演', '类型', '豆瓣评分', '评价人数'])
        writer.writerows(data)
        print("已成功写入 %s 条点击数据到 %s 文件中" % (len(data), filename))

def main():
    movie_list = get_movie_list()
    data = []
    click_limit = 25
    count = 0

    for movie_item in movie_list:
        movie_info = get_movie_info(movie_item, headers)
        data.append(movie_info)

        count += 1
        if count >= click_limit:
            break

    write_to_csv(data)

if __name__ == "__main__":
    main()

```



#坑 成功解决requests 报错raise SSLError(e, request=request)_requests.exceptions.SSLError_ HTTPSConnectionPool

[csdn](https://blog.csdn.net/AAIT11/article/details/130075038)

> #### 问题描述
>
> 在使用requests调用https接口时，会遇到ssl证书报错
>
> ```
> raise SSLError(e, request=request)
> requests.exceptions.SSLError: HTTPSConnectionPool(host='v4.ketangpai.com', port=443): Max retries exceeded with url: /UserApi/login (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:992)')))
> 12
> ```
>
> #### 解决思路
>
> 因为有的网站需要验证书，比如：12306，
>  所以只需要加加上参数：verify=证书路径，或verify=Flase
>
> #### 解决方法
>
> 步骤1：
>  在requests请求时，加上参数Verify
>
> ```python
> res1 = requests.post(url=url1, data=data1,verify=False)
> 1
> ```
>
> 但是，加上参数后，会有警告，提示安全问题
>
> ```shell
> InsecureRequestWarning: Unverified HTTPS request is being made to host '127.0.0.1'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
> 1
> ```
>
> 如何忽略警告呢？
>  通过 urllib3官方文档可知，只需要添加
>
> ```shell
> import urllib3
> 
> urllib3.disable_warnings()
> 123
> ```
>
> 参考资料：https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings

